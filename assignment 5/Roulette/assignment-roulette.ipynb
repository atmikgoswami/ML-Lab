{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gym gym-legacy-toytext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:04:52.765578Z","iopub.execute_input":"2025-11-10T17:04:52.766299Z","iopub.status.idle":"2025-11-10T17:04:56.154471Z","shell.execute_reply.started":"2025-11-10T17:04:52.766271Z","shell.execute_reply":"2025-11-10T17:04:56.153762Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\nCollecting gym-legacy-toytext\n  Downloading gym_legacy_toytext-0.0.5-py3-none-any.whl.metadata (938 bytes)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.2)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->gym) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.0->gym) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.0->gym) (2024.2.0)\nDownloading gym_legacy_toytext-0.0.5-py3-none-any.whl (9.9 kB)\nInstalling collected packages: gym-legacy-toytext\nSuccessfully installed gym-legacy-toytext-0.0.5\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import gym\nimport gym_toytext  \nimport numpy as np\n\nenv = gym.make(\"Roulette-v0\")\n\nnum_actions = env.action_space.n\nq_table = np.zeros(num_actions)\n\nlearning_rate = 0.1\ndiscount = 0.9\nepochs = 50_000\nepsilon = 1.0\nepsilon_decay = 0.9999\nepsilon_min = 0.01\n\nfor epoch in range(epochs):\n    env.reset()  \n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action = env.action_space.sample()\n    else:\n        action = np.argmax(q_table)\n\n    new_state, reward, done, info = env.step(action)\n\n    # Update Q-table\n    current_q = q_table[action]\n    new_q = current_q + learning_rate * (reward - current_q)\n    q_table[action] = new_q\n\n    # Decay epsilon\n    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n\nenv.close()\n\nprint(\"\\n Training finished!\")\nprint(\"Final Q-values (expected reward for each bet):\")\nprint(q_table.round(4))\n\nbest_action = np.argmax(q_table)\nprint(f\"\\nBest bet to make: Action {best_action} (Expected reward: {q_table[best_action]:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:18:42.054535Z","iopub.execute_input":"2025-11-10T17:18:42.055144Z","iopub.status.idle":"2025-11-10T17:18:42.599710Z","shell.execute_reply.started":"2025-11-10T17:18:42.055123Z","shell.execute_reply":"2025-11-10T17:18:42.598829Z"}},"outputs":[{"name":"stdout","text":"\n Training finished!\nFinal Q-values (expected reward for each bet):\n[-0.9995 -0.3148 -0.0763 -0.0368 -0.1907 -0.005  -0.2966 -0.2057 -0.0795\n -0.0979 -0.5954 -0.5181 -0.1284 -0.1099 -0.0922 -0.2147 -0.2561 -0.2633\n -0.0645 -0.3251 -0.5733 -0.1829 -0.3116 -0.1107 -0.2354 -0.3633 -0.4979\n -0.1652 -0.0952 -0.1793 -0.178  -0.3516 -0.1185 -0.1108 -0.0329 -0.1754\n -0.1043  0.    ]\n\nBest bet to make: Action 37 (Expected reward: 0.00)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import gym\nimport gym_toytext\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\n\nenv = gym.make(\"Roulette-v0\")\nstate_size = 1  \naction_size = env.action_space.n\n\n# Q-network\nclass QNetwork(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(QNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, action_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nq_network = QNetwork(state_size, action_size).to(device)\noptimizer = optim.Adam(q_network.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# Replay buffer\nmemory = deque(maxlen=5000)\nbatch_size = 64\n\nepisodes = 50000\ngamma = 0.9\nepsilon = 1.0\nepsilon_min = 0.01\nepsilon_decay = 0.9999\n\n# Training loop\nfor ep in range(episodes):\n    env.reset()  \n    state = np.array([0.0], dtype=np.float32)  \n    done = False\n\n    # Epsilon-greedy \n    if np.random.rand() < epsilon:\n        action = env.action_space.sample()\n    else:\n        with torch.no_grad():\n            state_tensor = torch.tensor(state).unsqueeze(0).to(device)\n            action = torch.argmax(q_network(state_tensor)).item()\n\n    # Take action\n    next_state, reward, done, info = env.step(action)\n    next_state = np.array([0.0], dtype=np.float32)  \n\n    # Store in replay memory\n    memory.append((state, action, reward, next_state, done))\n\n    if len(memory) >= batch_size:\n        batch = random.sample(memory, batch_size)\n        states_b, actions_b, rewards_b, next_states_b, dones_b = zip(*batch)\n\n        states_b = torch.tensor(np.array(states_b), dtype=torch.float32).to(device)\n        next_states_b = torch.tensor(np.array(next_states_b), dtype=torch.float32).to(device)\n        actions_b = torch.tensor(np.array(actions_b)).unsqueeze(1).to(device)\n        rewards_b = torch.tensor(np.array(rewards_b), dtype=torch.float32).unsqueeze(1).to(device)\n        dones_b = torch.tensor(np.array(dones_b), dtype=torch.float32).unsqueeze(1).to(device)\n\n        # Compute Q targets\n        q_values = q_network(states_b).gather(1, actions_b)\n        with torch.no_grad():\n            q_next = q_network(next_states_b).max(1)[0].unsqueeze(1)\n            q_target = rewards_b + gamma * q_next * (1 - dones_b)\n\n        # Update network\n        loss = criterion(q_values, q_target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Decay epsilon\n    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n\nenv.close()\n\nwith torch.no_grad():\n    state_tensor = torch.tensor([[0.0]], dtype=torch.float32).to(device)\n    final_qs = q_network(state_tensor).cpu().numpy()[0]\n\nprint(\"\\n DRL Training finished!\")\nprint(\"Final Q-values (expected reward for each bet):\")\nprint(final_qs.round(4))\nbest_action = np.argmax(final_qs)\nprint(f\"\\n Best bet to make: Action {best_action} (Expected reward: {final_qs[best_action]:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:23:53.627718Z","iopub.execute_input":"2025-11-10T17:23:53.628000Z","iopub.status.idle":"2025-11-10T17:25:41.723023Z","shell.execute_reply.started":"2025-11-10T17:23:53.627980Z","shell.execute_reply":"2025-11-10T17:25:41.722025Z"}},"outputs":[{"name":"stdout","text":"\n DRL Training finished!\nFinal Q-values (expected reward for each bet):\n[-0.592   0.3222 -0.6101  0.4095 -0.1731  0.2514  0.4283  0.0149  0.3761\n  0.2129  0.3196  0.2982  0.3185  0.2362  0.3807  0.4156  0.2812  0.2258\n  0.1438  0.1237  0.2043  0.4185  0.3294  0.2778  0.295  -0.1847  0.0484\n -0.08    0.1471 -0.4359  0.2092  0.4015 -0.1454  0.3097  0.3672  0.2217\n -0.3565  0.    ]\n\n Best bet to make: Action 6 (Expected reward: 0.43)\n","output_type":"stream"}],"execution_count":26}]}