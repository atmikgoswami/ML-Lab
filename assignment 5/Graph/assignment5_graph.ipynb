{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfAoJ0jsaKXB",
        "outputId": "4428f2f4-c410-4e79-81da-679cab59927c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graphs = [\n",
        "    {\n",
        "        \"name\": \"G1\",\n",
        "        \"adj_matrix\": [\n",
        "            [0, 1, 0, 0],\n",
        "            [1, 0, 1, 0],\n",
        "            [0, 1, 0, 1],\n",
        "            [0, 0, 1, 0]\n",
        "        ],\n",
        "        \"start\": 0,\n",
        "        \"end\": 3\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"G2\",\n",
        "        \"adj_matrix\": [\n",
        "            # 0 1 2 3 4 5 6 7 8\n",
        "            [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
        "            [1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
        "            [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "            [0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
        "            [0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "            [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
        "        ],\n",
        "        \"start\": 0,\n",
        "        \"end\": 8\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "FKVWok2smBQP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []"
      ],
      "metadata": {
        "id": "mgtDXQbEoqto"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class GraphEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, adj_matrix, start_node, end_node):\n",
        "        super(GraphEnv, self).__init__()\n",
        "\n",
        "        self.adj_matrix = np.array(adj_matrix)\n",
        "        self.num_nodes = len(adj_matrix)\n",
        "        self.start_node = start_node\n",
        "        self.end_node = end_node\n",
        "\n",
        "        self.current_node = self.start_node\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.num_nodes)\n",
        "        self.observation_space = spaces.Discrete(self.num_nodes)\n",
        "\n",
        "        # Step counter to prevent infinite loops\n",
        "        self.max_steps = self.num_nodes * 2\n",
        "        self.step_count = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_node = self.start_node\n",
        "        self.step_count = 0\n",
        "        return self.current_node, {}\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.step_count += 1\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Check if neighbor\n",
        "        if self.adj_matrix[self.current_node][action] == 1:\n",
        "            self.current_node = action\n",
        "            reward = -1\n",
        "        else:\n",
        "            reward = -10\n",
        "\n",
        "        if self.current_node == self.end_node:\n",
        "            terminated = True\n",
        "            reward = 0\n",
        "\n",
        "        if self.step_count >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        return self.current_node, reward, terminated, truncated, {}"
      ],
      "metadata": {
        "id": "PiFQRZYWjpwX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q-Learning**"
      ],
      "metadata": {
        "id": "p1Gx4QBzj_zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for g in graphs:\n",
        "    adj_matrix = g[\"adj_matrix\"]\n",
        "    START_NODE = g[\"start\"]\n",
        "    END_NODE = g[\"end\"]\n",
        "\n",
        "    env = GraphEnv(adj_matrix, START_NODE, END_NODE)\n",
        "    q_table = np.zeros([env.num_nodes, env.num_nodes])\n",
        "\n",
        "    # RL hyperparameters\n",
        "    learning_rate = 0.1\n",
        "    discount = 0.95\n",
        "    epochs = 1000\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.99\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        state = env.reset()[0]\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "            if np.random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            old_q = q_table[state, action]\n",
        "            max_future_q = np.max(q_table[new_state])\n",
        "            q_table[state, action] = old_q + learning_rate * (reward + discount * max_future_q - old_q)\n",
        "            state = new_state\n",
        "\n",
        "        epsilon = max(0.01, epsilon * epsilon_decay)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Extract path\n",
        "    path = [START_NODE]\n",
        "    state = START_NODE\n",
        "    while state != END_NODE:\n",
        "        action = int(np.argmax(q_table[state]))\n",
        "        path.append(action)\n",
        "        state = action\n",
        "        if len(path) > env.num_nodes:\n",
        "            print(f\"Warning: RL stuck in loop on {g['name']}\")\n",
        "            break\n",
        "\n",
        "    # Store RL results\n",
        "    results.append({\n",
        "        \"graph\": g[\"name\"],\n",
        "        \"method\": \"RL\",\n",
        "        \"path\": path,\n",
        "        \"path_length\": len(path)-1,\n",
        "        \"training_time\": elapsed_time\n",
        "    })"
      ],
      "metadata": {
        "id": "l06CHIThoy33"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env = GraphEnv(adj_matrix, START_NODE, END_NODE)\n",
        "\n",
        "# q_table = np.zeros([env.num_nodes, env.num_nodes])\n",
        "\n",
        "# learning_rate = 0.1\n",
        "# discount = 0.95\n",
        "# epochs = 1000\n",
        "# epsilon = 1.0\n",
        "# epsilon_decay = 0.99\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     state = env.reset()[0]\n",
        "#     terminated = False\n",
        "#     truncated = False\n",
        "\n",
        "#     while not terminated and not truncated:\n",
        "#         # Epsilon-greedy\n",
        "#         if np.random.random() < epsilon:\n",
        "#             action = env.action_space.sample()\n",
        "#         else:\n",
        "#             action = np.argmax(q_table[state])\n",
        "\n",
        "#         new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "#         old_q = q_table[state, action]\n",
        "#         max_future_q = np.max(q_table[new_state])\n",
        "\n",
        "#         new_q = old_q + learning_rate * (reward + discount * max_future_q - old_q)\n",
        "#         q_table[state, action] = new_q\n",
        "\n",
        "#         state = new_state\n",
        "\n",
        "#     epsilon = max(0.01, epsilon * epsilon_decay)\n",
        "\n",
        "# print(\"RL Training finished!\")\n",
        "# print(\"Final Q-Table:\")\n",
        "# print(q_table.round(2))\n",
        "\n",
        "# path = [START_NODE]\n",
        "# state = START_NODE\n",
        "# while state != END_NODE:\n",
        "#     action = int(np.argmax(q_table[state]))\n",
        "#     path.append(action)\n",
        "#     state = action\n",
        "#     if len(path) > env.num_nodes:\n",
        "#         print(\"Error: Pathfinding stuck in a loop.\")\n",
        "#         break\n",
        "# print(f\"Shortest path found by RL: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wWECnXj8-_",
        "outputId": "33043378-6ae3-4eb2-a43a-53172b91d4b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RL Training finished!\n",
            "Final Q-Table:\n",
            "[[-11.19  -2.85  -2.85 -11.31 -10.86 -11.08 -10.49 -11.74 -11.97]\n",
            " [ -2.4  -10.95 -10.39  -2.38  -1.95 -10.07 -11.06 -10.43 -11.11]\n",
            " [ -2.85  -7.87  -8.73  -8.88  -1.95  -8.48  -9.53  -6.96  -9.47]\n",
            " [ -8.24  -2.31  -4.1   -7.83  -6.02  -7.54  -2.29  -2.33  -7.25]\n",
            " [ -9.03  -2.28  -2.49  -8.34  -9.55  -1.    -9.19  -9.56  -8.38]\n",
            " [ -7.94  -8.15  -9.11  -7.94  -1.6   -7.18  -8.5   -7.71   0.  ]\n",
            " [ -5.63  -3.63  -2.89  -2.3   -6.25  -3.66  -2.89  -2.92  -3.57]\n",
            " [ -6.5   -3.6   -2.99  -2.35  -4.2   -4.21  -5.32  -4.71  -6.27]\n",
            " [  0.     0.     0.     0.     0.     0.     0.     0.     0.  ]]\n",
            "Shortest path found by RL: [0, 1, 4, 5, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DRL**"
      ],
      "metadata": {
        "id": "9x2gPv39k0fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# --- Q-Network ---\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# --- Replay Buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float()\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# --- Helper function: one-hot encoding of node ---\n",
        "def one_hot(node, num_nodes):\n",
        "    vec = np.zeros(num_nodes, dtype=float)\n",
        "    vec[node] = 1.0\n",
        "    return vec\n",
        "\n",
        "# --- DRL parameters ---\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.95\n",
        "LR = 0.001\n",
        "EPSILON = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "EPISODES = 500\n",
        "UPDATE_EVERY = 4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- DRL for all graphs ---\n",
        "for g in graphs:\n",
        "    adj_matrix = g[\"adj_matrix\"]\n",
        "    START_NODE = g[\"start\"]\n",
        "    END_NODE = g[\"end\"]\n",
        "\n",
        "    env = GraphEnv(adj_matrix, START_NODE, END_NODE)\n",
        "    state_size = env.num_nodes\n",
        "    action_size = env.num_nodes\n",
        "\n",
        "    # Q-networks and optimizer\n",
        "    q_local = QNetwork(state_size, action_size).to(device)\n",
        "    q_target = QNetwork(state_size, action_size).to(device)\n",
        "    optimizer = optim.Adam(q_local.parameters(), lr=LR)\n",
        "    memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "    q_target.load_state_dict(q_local.state_dict())\n",
        "\n",
        "    epsilon = EPSILON\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Training loop ---\n",
        "    for episode in range(1, EPISODES + 1):\n",
        "        state = env.reset()[0]\n",
        "        state_vec = one_hot(state, state_size)\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "            state_tensor = torch.from_numpy(state_vec).float().unsqueeze(0).to(device)\n",
        "            q_local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = q_local(state_tensor)\n",
        "            q_local.train()\n",
        "\n",
        "            if random.random() > epsilon:\n",
        "                action = np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                action = random.choice(np.arange(action_size))\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state_vec = one_hot(next_state, state_size)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            memory.add(state_vec, action, reward, next_state_vec, done)\n",
        "\n",
        "            if len(memory) > BATCH_SIZE:\n",
        "                experiences = memory.sample()\n",
        "\n",
        "                # --- Learn function ---\n",
        "                states, actions, rewards, next_states, dones = experiences\n",
        "                states = states.to(device)\n",
        "                actions = actions.to(device)\n",
        "                rewards = rewards.to(device)\n",
        "                next_states = next_states.to(device)\n",
        "                dones = dones.to(device)\n",
        "\n",
        "                best_actions = q_local(next_states).detach().argmax(1).unsqueeze(1)\n",
        "                q_targets_next = q_target(next_states).detach().gather(1, best_actions)\n",
        "                q_targets = rewards + GAMMA * q_targets_next * (1 - dones)\n",
        "                q_expected = q_local(states).gather(1, actions)\n",
        "\n",
        "                loss = nn.MSELoss()(q_expected, q_targets)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Soft update\n",
        "                for target_param, local_param in zip(q_target.parameters(), q_local.parameters()):\n",
        "                    target_param.data.copy_(0.01 * local_param.data + (1.0 - 0.01) * target_param.data)\n",
        "\n",
        "            state_vec = next_state_vec\n",
        "\n",
        "        epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # --- Extract shortest path ---\n",
        "    path = [START_NODE]\n",
        "    state_vec = one_hot(START_NODE, state_size)\n",
        "    state = START_NODE\n",
        "    while state != END_NODE:\n",
        "        state_tensor = torch.from_numpy(state_vec).float().unsqueeze(0).to(device)\n",
        "        q_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = torch.argmax(q_local(state_tensor)).item()\n",
        "        path.append(action)\n",
        "        state_vec = one_hot(action, state_size)\n",
        "        state = action\n",
        "        if len(path) > env.num_nodes:\n",
        "            print(f\"Warning: DRL stuck in loop on {g['name']}\")\n",
        "            break\n",
        "\n",
        "    # --- Store DRL results ---\n",
        "    results.append({\n",
        "        \"graph\": g[\"name\"],\n",
        "        \"method\": \"DRL\",\n",
        "        \"path\": path,\n",
        "        \"path_length\": len(path)-1,\n",
        "        \"training_time\": elapsed_time\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3eoHbShpdrP",
        "outputId": "046302d0-74f6-4416-c198-b55e6cd054a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "table_data = []\n",
        "for r in results:\n",
        "    table_data.append([\n",
        "        r['graph'],\n",
        "        r['method'],\n",
        "        str(r['path']),\n",
        "        r['path_length'],\n",
        "        f\"{r['training_time']:.2f}s\"\n",
        "    ])\n",
        "\n",
        "headers = [\"Graph\", \"Method\", \"Path\", \"Path Length\", \"Training Time\"]\n",
        "\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqEh4p5AptYw",
        "outputId": "afb6805f-0bb9-4c51-e980-dc29bfbe2f5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----------------+---------------+-----------------+\n",
            "| Graph   | Method   | Path            |   Path Length | Training Time   |\n",
            "+=========+==========+=================+===============+=================+\n",
            "| G1      | RL       | [0, 1, 2, 3]    |             3 | 0.03s           |\n",
            "+---------+----------+-----------------+---------------+-----------------+\n",
            "| G2      | RL       | [0, 1, 4, 5, 8] |             4 | 0.05s           |\n",
            "+---------+----------+-----------------+---------------+-----------------+\n",
            "| G1      | DRL      | [0, 1, 2, 3]    |             3 | 7.59s           |\n",
            "+---------+----------+-----------------+---------------+-----------------+\n",
            "| G2      | DRL      | [0, 2, 4, 5, 8] |             4 | 13.01s          |\n",
            "+---------+----------+-----------------+---------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import numpy as np\n",
        "# import random\n",
        "# from collections import deque\n",
        "\n",
        "# # Q-Network\n",
        "# class QNetwork(nn.Module):\n",
        "#     def __init__(self, state_size, action_size):\n",
        "#         super(QNetwork, self).__init__()\n",
        "#         self.fc1 = nn.Linear(state_size, 64)\n",
        "#         self.fc2 = nn.Linear(64, 64)\n",
        "#         self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "#         return self.fc3(x)\n",
        "\n",
        "# # Replay Buffer\n",
        "# class ReplayBuffer:\n",
        "#     def __init__(self, buffer_size, batch_size):\n",
        "#         self.memory = deque(maxlen=buffer_size)\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#     def add(self, state, action, reward, next_state, done):\n",
        "#         self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "#     def sample(self):\n",
        "#         experiences = random.sample(self.memory, self.batch_size)\n",
        "#         states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
        "#         actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
        "#         rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
        "#         next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float()\n",
        "#         dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
        "#         return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.memory)\n",
        "\n",
        "# env = GraphEnv(adj_matrix, START_NODE, END_NODE)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# state_size = env.num_nodes\n",
        "# action_size = env.num_nodes\n",
        "\n",
        "# q_local = QNetwork(state_size, action_size).to(device)\n",
        "# q_target = QNetwork(state_size, action_size).to(device)\n",
        "# optimizer = optim.Adam(q_local.parameters(), lr=0.001)\n",
        "# memory = ReplayBuffer(10000, 32)\n",
        "# q_target.load_state_dict(q_local.state_dict())\n",
        "\n",
        "# GAMMA = 0.95\n",
        "# EPSILON = 1.0\n",
        "# EPSILON_MIN = 0.01\n",
        "# EPSILON_DECAY = 0.995\n",
        "# UPDATE_EVERY = 4\n",
        "# episodes = 500\n",
        "\n",
        "# def one_hot(node, num_nodes):\n",
        "#     vec = np.zeros(num_nodes, dtype=float)\n",
        "#     vec[node] = 1.0\n",
        "#     return vec\n",
        "\n",
        "# def learn(experiences, gamma):\n",
        "#     states, actions, rewards, next_states, dones = experiences\n",
        "#     states = states.to(device)\n",
        "#     actions = actions.to(device)\n",
        "#     rewards = rewards.to(device)\n",
        "#     next_states = next_states.to(device)\n",
        "#     dones = dones.to(device)\n",
        "\n",
        "#     best_actions = q_local(next_states).detach().argmax(1).unsqueeze(1)\n",
        "#     q_targets_next = q_target(next_states).detach().gather(1, best_actions)\n",
        "#     q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
        "#     q_expected = q_local(states).gather(1, actions)\n",
        "\n",
        "#     loss = nn.MSELoss()(q_expected, q_targets)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # Soft update\n",
        "#     for target_param, local_param in zip(q_target.parameters(), q_local.parameters()):\n",
        "#         target_param.data.copy_(0.01 * local_param.data + (1.0 - 0.01) * target_param.data)\n",
        "\n",
        "# # Training loop\n",
        "# for episode in range(1, episodes + 1):\n",
        "#     state = env.reset()[0]\n",
        "#     state_vec = one_hot(state, state_size)\n",
        "#     terminated = False\n",
        "#     truncated = False\n",
        "\n",
        "#     while not terminated and not truncated:\n",
        "#         state_tensor = torch.from_numpy(state_vec).float().unsqueeze(0).to(device)\n",
        "#         q_local.eval()\n",
        "#         with torch.no_grad():\n",
        "#             action_values = q_local(state_tensor)\n",
        "#         q_local.train()\n",
        "\n",
        "#         if random.random() > EPSILON:\n",
        "#             action = np.argmax(action_values.cpu().data.numpy())\n",
        "#         else:\n",
        "#             action = random.choice(np.arange(action_size))\n",
        "\n",
        "#         next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "#         next_state_vec = one_hot(next_state, state_size)\n",
        "#         done = terminated or truncated\n",
        "\n",
        "#         memory.add(state_vec, action, reward, next_state_vec, done)\n",
        "\n",
        "#         if len(memory) > 32:\n",
        "#             experiences = memory.sample()\n",
        "#             learn(experiences, GAMMA)\n",
        "\n",
        "#         state_vec = next_state_vec\n",
        "\n",
        "#     EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
        "\n",
        "#     if episode % 50 == 0:\n",
        "#         print(f\"Episode {episode}/{episodes}\")\n",
        "\n",
        "# path = [START_NODE]\n",
        "# state_vec = one_hot(START_NODE, state_size)\n",
        "# state = START_NODE\n",
        "# while state != END_NODE:\n",
        "#     state_tensor = torch.from_numpy(state_vec).float().unsqueeze(0).to(device)\n",
        "#     q_local.eval()\n",
        "#     with torch.no_grad():\n",
        "#         action = torch.argmax(q_local(state_tensor)).item()\n",
        "#     path.append(action)\n",
        "#     state_vec = one_hot(action, state_size)\n",
        "#     state = action\n",
        "#     if len(path) > env.num_nodes:\n",
        "#         print(\"Stuck in loop!\")\n",
        "#         break\n",
        "\n",
        "# print(f\"Shortest path found by DRL: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yQij0aDk55P",
        "outputId": "039e4e98-0bfc-4572-b5f5-a1c41801e2b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Episode 50/500\n",
            "Episode 100/500\n",
            "Episode 150/500\n",
            "Episode 200/500\n",
            "Episode 250/500\n",
            "Episode 300/500\n",
            "Episode 350/500\n",
            "Episode 400/500\n",
            "Episode 450/500\n",
            "Episode 500/500\n",
            "Shortest path found by DRL: [0, 2, 4, 5, 8]\n"
          ]
        }
      ]
    }
  ]
}