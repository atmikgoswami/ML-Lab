{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T13:52:50.137591Z","iopub.execute_input":"2025-11-10T13:52:50.138344Z","iopub.status.idle":"2025-11-10T13:52:53.326276Z","shell.execute_reply.started":"2025-11-10T13:52:50.138311Z","shell.execute_reply":"2025-11-10T13:52:53.325210Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# **Mountain Car RL**","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym \nimport numpy as np\n\nenv = gym.make('MountainCar-v0')\n\n# Discretize \npos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], 20)\nvel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], 20)\n\nq_table = np.zeros((len(pos_space), len(vel_space), env.action_space.n))\n\nlearning_rate = 0.1\ndiscount = 0.95\nepochs = 2000\nepsilon = 0.5\nepsilon_decay = 0.998\n\ndef get_discrete_state(state):\n    pos, vel = state\n    pos_bin = np.digitize(pos, pos_space)\n    vel_bin = np.digitize(vel, vel_space)\n    return (pos_bin, vel_bin)\n\n# Training loop\nfor epoch in range(epochs):\n    state = get_discrete_state(env.reset()[0]) \n    terminated = False\n    truncated = False\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch}\")\n\n    while not terminated and not truncated:\n        # Epsilon-greedy action selection\n        if np.random.random() < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table[state])\n            \n        new_state_continuous, reward, terminated, truncated, _ = env.step(action) \n        new_state = get_discrete_state(new_state_continuous)\n        \n        done = terminated or truncated \n\n        if not done:\n            max_future_q = np.max(q_table[new_state])\n            current_q = q_table[state + (action,)]\n            \n            new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)\n            q_table[state + (action,)] = new_q\n        \n        elif terminated:\n            q_table[state + (action,)] = 0  \n\n        state = new_state\n        \n    if epsilon > 0.05:\n        epsilon *= epsilon_decay\n\nprint(\"Training finished!\")\nenv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T13:53:14.618184Z","iopub.execute_input":"2025-11-10T13:53:14.618857Z","iopub.status.idle":"2025-11-10T13:53:31.166729Z","shell.execute_reply.started":"2025-11-10T13:53:14.618824Z","shell.execute_reply":"2025-11-10T13:53:31.165871Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0\nEpoch: 100\nEpoch: 200\nEpoch: 300\nEpoch: 400\nEpoch: 500\nEpoch: 600\nEpoch: 700\nEpoch: 800\nEpoch: 900\nEpoch: 1000\nEpoch: 1100\nEpoch: 1200\nEpoch: 1300\nEpoch: 1400\nEpoch: 1500\nEpoch: 1600\nEpoch: 1700\nEpoch: 1800\nEpoch: 1900\nTraining finished!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import imageio\nimport numpy as np\nimport gymnasium as gym\n\nvideo_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n\nframes = []\nstate = get_discrete_state(video_env.reset()[0])\ndone = False\n\nwhile not done:\n    action = np.argmax(q_table[state])\n    new_state_continuous, _, terminated, truncated, _ = video_env.step(action)\n    state = get_discrete_state(new_state_continuous)\n    done = terminated or truncated\n    frame = video_env.render()\n    frame = np.array(frame)\n    if frame.ndim == 4 and frame.shape[0] == 2:\n        frame = frame[0]\n    frames.append(frame)\n\nvideo_env.close()\n\nh, w = frames[0].shape[:2]\nframes = [f[:h, :w, :3] for f in frames]\n\n# Save video\nvideo_path = \"mountaincar_RL.mp4\"\nimageio.mimsave(video_path, frames, fps=30, macro_block_size=None)\nprint(f\"Video saved as: {video_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:01:29.880726Z","iopub.execute_input":"2025-11-10T14:01:29.881617Z","iopub.status.idle":"2025-11-10T14:01:30.944338Z","shell.execute_reply.started":"2025-11-10T14:01:29.881587Z","shell.execute_reply":"2025-11-10T14:01:30.943486Z"}},"outputs":[{"name":"stdout","text":"Video saved as: mountaincar_RL.mp4\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# **Mountain Car DRL**","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\n# Q-Network \nclass QNetwork(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(QNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, action_size)\n    \n    def forward(self, state):\n        x = torch.relu(self.fc1(state))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n# Replay Buffer\nclass ReplayBuffer:\n    def __init__(self, buffer_size, batch_size):\n        self.memory = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n    \n    def add(self, state, action, reward, next_state, done): # 'done' here means (terminated or truncated)\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def sample(self):\n        experiences = random.sample(self.memory, self.batch_size)\n        \n        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float()\n        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long()\n        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float()\n        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float()\n        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float()\n        \n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nBUFFER_SIZE = 100000\nBATCH_SIZE = 64\nGAMMA = 0.99\nLR = 0.0005\nUPDATE_EVERY = 4\n\nenv = gym.make('MountainCar-v0')\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\n\nq_network_local = QNetwork(state_size, action_size).to(device)\nq_network_target = QNetwork(state_size, action_size).to(device)\noptimizer = optim.Adam(q_network_local.parameters(), lr=LR)\nmemory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n\nq_network_target.load_state_dict(q_network_local.state_dict())\n\ndef learn(experiences, gamma):\n    states, actions, rewards, next_states, dones = experiences\n\n    states = states.to(device)\n    actions = actions.to(device)\n    rewards = rewards.to(device)\n    next_states = next_states.to(device)\n    dones = dones.to(device)\n\n    best_actions = q_network_local(next_states).detach().argmax(1).unsqueeze(1)\n    q_targets_next = q_network_target(next_states).detach().gather(1, best_actions)\n    q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n    q_expected = q_network_local(states).gather(1, actions)\n\n    loss = nn.MSELoss()(q_expected, q_targets)\n\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(q_network_local.parameters(), 1)\n    optimizer.step()\n\n    # Soft update\n    for target_param, local_param in zip(q_network_target.parameters(), q_network_local.parameters()):\n        target_param.data.copy_(0.01 * local_param.data + (1.0 - 0.01) * target_param.data)\n\n# Training Loop\nepisodes = 1200\nmax_t = 1000 \nepsilon = 1.0\nepsilon_decay = 0.997\nepsilon_min = 0.01\n\nscores = []\nscores_window = deque(maxlen=100)\n\nfor i_episode in range(1, episodes + 1):\n    state = env.reset()[0] \n    score = 0\n    \n    for t in range(max_t):\n        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        q_network_local.eval()\n        with torch.no_grad():\n            action_values = q_network_local(state_tensor)\n        q_network_local.train()\n        \n        if random.random() > epsilon:\n            action = np.argmax(action_values.cpu().data.numpy())\n        else:\n            action = random.choice(np.arange(action_size))\n            \n        next_state, reward, terminated, truncated, _ = env.step(action)\n\n        # Reward shaping\n        position, velocity = next_state\n        reward = reward + 0.5 * abs(velocity) + (position + 0.5)\n        done = terminated or truncated \n        \n        memory.add(state, action, reward, next_state, done)\n        \n        if len(memory) > BATCH_SIZE and t % UPDATE_EVERY == 0:\n            experiences = memory.sample()\n            learn(experiences, GAMMA)\n            \n        state = next_state\n        score += reward\n        if done:\n            break\n            \n    scores_window.append(score)\n    scores.append(score)\n    \n    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n    \n    if i_episode % 100 == 0:\n        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n\nprint('Training finished!')\nenv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:39:15.224585Z","iopub.execute_input":"2025-11-10T14:39:15.225306Z","iopub.status.idle":"2025-11-10T14:43:32.145988Z","shell.execute_reply.started":"2025-11-10T14:39:15.225270Z","shell.execute_reply":"2025-11-10T14:43:32.145285Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEpisode 100\tAverage Score: -201.43\nEpisode 200\tAverage Score: -194.04\nEpisode 300\tAverage Score: -191.25\nEpisode 400\tAverage Score: -191.03\nEpisode 500\tAverage Score: -183.16\nEpisode 600\tAverage Score: -181.69\nEpisode 700\tAverage Score: -180.63\nEpisode 800\tAverage Score: -165.29\nEpisode 900\tAverage Score: -157.39\nEpisode 1000\tAverage Score: -139.33\nEpisode 1100\tAverage Score: -134.86\nEpisode 1200\tAverage Score: -126.49\nTraining finished!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import imageio\n\nrender_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n\nframes = []\nstate = render_env.reset()[0]\ndone = False\n\nwhile not done:\n    state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n    \n    q_network_local.eval()\n    with torch.no_grad():\n        action_values = q_network_local(state_tensor)\n    action = np.argmax(action_values.cpu().data.numpy())\n    \n    next_state, reward, terminated, truncated, _ = render_env.step(action)\n    done = terminated or truncated\n    \n    frame = render_env.render()\n    frames.append(frame)\n    \n    state = next_state\n\nrender_env.close()\n\n# Save the video\nvideo_path = \"mountaincar_DRL.mp4\"\nimageio.mimsave(video_path, frames, fps=30, macro_block_size=None)\nprint(f\"Video saved as: {video_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:52:24.593284Z","iopub.execute_input":"2025-11-10T14:52:24.593937Z","iopub.status.idle":"2025-11-10T14:52:25.311828Z","shell.execute_reply.started":"2025-11-10T14:52:24.593904Z","shell.execute_reply":"2025-11-10T14:52:25.311009Z"}},"outputs":[{"name":"stderr","text":"error: XDG_RUNTIME_DIR not set in the environment.\n","output_type":"stream"},{"name":"stdout","text":"Video saved as: mountaincar_DRL.mp4\n","output_type":"stream"}],"execution_count":29}]}